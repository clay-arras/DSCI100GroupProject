{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd9992a9-d85b-486d-b5bf-efbe0c8394a1",
   "metadata": {},
   "source": [
    "\n",
    "# Diagnosing Heart Disease of Patient's based on Age, Resting Blood Pressure, Cholesterol, and Max Heart Rate.\n",
    "\n",
    "## Group 30: Toby Lau, Neil Lin, Tanish Gupta, Kristy Fielding\n",
    "\n",
    "### Background Information & Introduction\n",
    "\n",
    "Heart disease is one of the leading causes of death around the world that is caused by a wide variety of factors. Common heart diseases include coronary heart disease, arrhythmias, strokes, etc. Throughout the 21st century, medical professionals have deduced how certain factors may give leeway to one having a certain type of heart disease and the extent of its danger to the human body (Centers for Disease Control and Prevention, 2023). In this project, we will try to determine whether a patient has heart disease given the relevant predictors. The dataset being used is a combination of datasets of patients who were suspected to have heart disease from Cleveland and Switzerland. The dataset contains 14 attributes that could contribute to a patient's heart disease. Of the 14, we hope to train a model that is able to classify these stages given some attributes.\n",
    "\n",
    "**Research Question: How can age, resting blood pressure, cholesterol, and max heart rate diagnose heart disease in patients from the West?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559e1dc6-47dd-4fe1-abb4-77783df76924",
   "metadata": {},
   "source": [
    "\n",
    "## Methods\n",
    "To create the model, we will be combining two datasets from the same source so that we have more data to train and test on. In doing so, we capture a greater variety of data as patients from different parts of the world also have different lifestyles.\n",
    "\n",
    "### Data reading and wrangling\n",
    "1. Import the required libraries and load both datasets from the web.\n",
    "3. Assign the correct column names and types to the dataset\n",
    "4. Combine both datasets.\n",
    "5. Wrangle the dataset into a tidy format.\n",
    "6. Split the data into training and testing sets\n",
    "7. Summarize and visualize data\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "1. Set a seed.\n",
    "2. Create a recipe that defines the \"stage\" column as our response variable and \"age\", \"restbps\", \"chol\", and \"thalach\" columns as predictors.\n",
    "3. Standardize predictors using step_scale and step_center on all predictors.\n",
    "4. Create a $k$-nn model  with the neighbors = tune() parameter.\n",
    "5. Create a tibble with a single neighbors column containing numbers from 1 to 100.\n",
    "6. Perform Cross-validation on 10 folds to tune the model and find the best $k$ value.\n",
    "7. Plot the accuracy of the model against the number of neighbors to determine the $k$ value with the highest accuracy.\n",
    "8. Using the $k$ value that obtained the highest accuracy, create a new $k$-nn model\n",
    "\n",
    "### Evaluating\n",
    "1. Test the model using our testing data set.\n",
    "2. Check for over/underfitting.\n",
    "3. Create a confusion matrix\n",
    "4. Assess the model's performance using tidymodels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde58f5-d737-4a5d-a308-1a07b1364be9",
   "metadata": {},
   "source": [
    "### Data reading and wrangling\n",
    "First, we import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04538702-0674-4291-94e2-c870519b3516",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(GGally)\n",
    "library(tidymodels)\n",
    "install.packages(\"themis\")\n",
    "install.packages(\"yardstick\")\n",
    "library(themis)\n",
    "library(yardstick)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036da5e7-5c2b-4c35-bc64-8a991727a8d9",
   "metadata": {},
   "source": [
    "Then we read in both our datasets. They come from the same source but have different URLS. We know from the data source the name of each column thanks to a variable list, so we can read use \"read_csv\" with the col_names argument set to a list of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64694803-33b7-47a3-b115-b847cc0003ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data from URL\n",
    "cleveland_data <- read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\",\n",
    "                          col_names = c(\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"))\n",
    "hungarian_data <- read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\",\n",
    "                          col_names = c(\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"))\n",
    "\n",
    "# previewing the data\n",
    "head(cleveland_data) \n",
    "head(hungarian_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f7193-3909-4fe7-bb8e-f5e90c6a7175",
   "metadata": {},
   "source": [
    "Figure 1\\\n",
    "Looking at figure 1, we can notice that some of the columns which are supposed to be exactly the same have different types. The data source also tells us the correct type of each column so we can mutate both datasets accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb36e748-ce8b-486f-af05-ca91c2a2a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutating all the data \n",
    "cleveland_data <- cleveland_data |> \n",
    "    mutate(age = as.integer(age), sex = as.factor(sex), cp = as.factor(cp), trestbps = as.integer(trestbps), \n",
    "    chol = as.integer(chol), fbs = as.factor(fbs), restecg = as.factor(restecg), thalach = as.integer(thalach),\n",
    "    exang = as.factor(exang), oldpeak = as.integer(oldpeak), slope = as.factor(slope), ca = as.integer(ca), \n",
    "    thal = as.factor(thal), num = as.integer(num))\n",
    "\n",
    "hungarian_data <- hungarian_data |> \n",
    "    mutate(age = as.integer(age), sex = as.factor(sex), cp = as.factor(cp), trestbps = as.integer(trestbps), \n",
    "    chol = as.integer(chol), fbs = as.factor(fbs), restecg = as.factor(restecg),  thalach = as.integer(thalach),\n",
    "    exang = as.factor(exang), oldpeak = as.integer(oldpeak), slope = as.factor(slope), ca = as.integer(ca), \n",
    "    thal = as.factor(thal), num = as.integer(num))\n",
    "\n",
    "# what the new data looks like\n",
    "head(cleveland_data)\n",
    "head(hungarian_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f4a91-7c8e-405c-b74f-f2302ba27811",
   "metadata": {},
   "source": [
    "Figure 2\\\n",
    "Although unclean, the datasets in figure 2 should have the same type in each column which allows us to combine the two. We will call this new resulting dataset \"data_all\". We also don't need every single column the dataset provides, so we will also select only the ones we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2aa732-f590-404a-90f1-c09b0c03bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all <- bind_rows(cleveland_data, hungarian_data) |> \n",
    "    select(age, trestbps, chol, thalach, -ca, -thal, num)\n",
    "\n",
    "head(data_all)\n",
    "nrow(data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f24d7-4b79-4111-991d-4064f0091bae",
   "metadata": {},
   "source": [
    "Figure 3\\\n",
    "With all our data in one tibble, we can move on to the actual cleaning. First we see whether there are any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf24be-4303-435a-8ca5-c956716cea79",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nulls <- data_all |> \n",
    "    map_df(\\(c) sum(is.na(c)))\n",
    "\n",
    "nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084a466-3768-4871-a9af-67bb1f4405b4",
   "metadata": {},
   "source": [
    "Figure 4\\\n",
    "According to the figure above, there are still a few null values in the \"trestbps\", \"chol\", and \"thalach\" columns. So we must filter those values out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec06e4-4361-47dd-8efb-6ad1b07e3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean <- data_all |> \n",
    "    filter(!is.na(trestbps)) |> \n",
    "    filter(!is.na(chol)) |> \n",
    "    filter(!is.na(thalach))\n",
    "\n",
    "nrow(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f402d86-1a93-40a7-a9b5-3407079bc2ab",
   "metadata": {},
   "source": [
    "This leaves us with 573 rows in the dataset left but this should still be sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f708b9-78be-4421-a458-550a375ac09d",
   "metadata": {},
   "source": [
    "Now, we decided to count the number of observations for each \"num\" category. To check how balanced the dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebfbe94-c2e4-4d80-9b4c-de5d9fc22670",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width = 6, repr.plot.height=5)\n",
    "\n",
    "summary <- data_all |> \n",
    "    group_by(num) |> \n",
    "    summarize(count = n())\n",
    "\n",
    "summary_plot <- summary |> \n",
    "    ggplot(aes(num, count)) +\n",
    "    geom_bar(stat = \"identity\") +\n",
    "    labs(x = \"Value in `num` column\", y = \"Number of observations\") +\n",
    "    ggtitle(\"Figure 5: Number of Observations per Factor in Num Column\")\n",
    "\n",
    "summary_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86c27bb-867f-4ce1-85b5-20fd64f6c618",
   "metadata": {},
   "source": [
    "Figure 5\\\n",
    "Unfortunately, according to figure 5, our dataset is extremely imbalanced. So we've decided to create a new class that merges values of num from 1 to 4. \n",
    "\n",
    "According to the data source, values of 0 in the num column represent patients with <50% vessel diameter narrowing while values > 0 in the num column represent patients with >50% vessel diameter narrowing. Since mild heart disease is classified as <50% narrowing, moderate heart disease is classified as 50-80% narrowing, and severe heart disease is 80-100% narrowing (Cleveland Clinic). We can bucket the \"num\" values into respective \"mild\", \"moderate\", and \"severe\" classes into a new \"stage\" column. However, due to the class imbalance, we have decided to combine the \"moderate\" and \"severe\" classes. This corresponds to num values of 1-4. The following table represents how the new \"stage\" column relates to \"num\".\n",
    "\n",
    "| Num value | Percentage blockage | Stage |\n",
    "| -------- | ------- | ----|\n",
    "| 0 | < 50% | Mild |\n",
    "| 1-4 | > 50% | Moderate/Severe |\n",
    "\n",
    "With only two classes, we will perform a binary classification instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68380170-0696-4a82-8861-5d2285417a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width = 7, repr.plot.height=5)\n",
    "\n",
    "data_clean <- data_clean |> \n",
    "    mutate(stage = cut(num, breaks=c(-Inf,0,4), labels=c(\"Mild\", \"Moderate/Severe\")))\n",
    "head(data_clean)\n",
    "\n",
    "data_clean_summary <- data_clean |> \n",
    "    group_by(stage) |> \n",
    "    summarize(count = n()) |> \n",
    "    ggplot(aes(stage, count)) +\n",
    "    geom_bar(stat = \"identity\") +\n",
    "    labs(x = \"Stage of Heart Disease\", y = \"Number of Observations\") +\n",
    "    ggtitle(\"Figure 6: Number of observations per Factor in Stage Column\")\n",
    "\n",
    "data_clean_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f62629-17d0-4d66-b894-44b1341f8979",
   "metadata": {},
   "source": [
    "Figure 7\\\n",
    "By combining the classes, we improve the balance of the classes slightly but not as much as we would like. We will most likely have to upsample when creating the model's recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad22e01-a4b7-47f4-b974-5c5fb546a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(69)\n",
    "\n",
    "split <- initial_split(data_clean, prop=0.75, strata = stage)\n",
    "data_training <- training(split)\n",
    "data_testing <- testing(split)\n",
    "\n",
    "n_training <- nrow(data_training)\n",
    "n_testing <- nrow(data_testing)\n",
    "nrow(data_training)\n",
    "nrow(data_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9543edb-e7c9-41e1-8690-63672be81ecb",
   "metadata": {},
   "source": [
    "Splitting our data into training and testing datasets using initial_split will ensure that bias and order were removed. There are 429 rows (75%) in the training set and 144 rows (25%) in the testing set. With this respective proportion, we will have enough data to sufficiently train our model and then utilize the testing set to evaluate its performance on unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e0c857-a772-4cab-95ba-6d95d92c9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_summary <- data_training |> \n",
    "    group_by(stage) |> \n",
    "    summarize(count = n(), Prop = (n()/n_training)*100)\n",
    "\n",
    "training_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da9bfb-4329-4e61-a005-c1a601b04975",
   "metadata": {},
   "source": [
    "Figure 8\\\n",
    "Within the training dataset, 58% of the data belongs to the \"Mild\" class while 46% of the data belongs to the \"Moderate/Severe\" class. In addition to the upsampling later, this should sufficiently reduce the bias in our model. To make sure the two classes are distinguishable we tabulate some each class' properties below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b9e76-333c-4928-93c4-656d503d307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_means <- data_training |> \n",
    "    group_by(stage) |> \n",
    "    summarize(mean_age = mean(age), mean_trestbps = mean(trestbps), mean_chol = mean(chol), mean_thalach = mean(thalach))\n",
    "\n",
    "training_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16999c1f-33d4-443a-bf98-616014819f31",
   "metadata": {},
   "source": [
    "Figure 9\\\n",
    "According to the table above, there is quite a considerable difference in the mean of each predictor for each class. It shows that patients in the \"Moderate/Severe\" stage are older, have a higher resting heart rate, and higher levels of cholesterol but a lower max heart rate while the opposite is true for patients in the \"Mild\" stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e2c31-2e0d-4aad-bc13-e595e707ac7a",
   "metadata": {},
   "source": [
    "### Summarizing and Visualizing the Data\n",
    "With our training set, we can summarize and visualize our training data to uncover any underlying patterns in relation to our new \"stage\" column. The following code represents a ggpairs plot on our training data set that plots each predictor against each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7983f0-49ef-415a-bbe4-0da027522c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(options.repr.width = 10, options.repr.height = 10)\n",
    "\n",
    "ggpair <- ggpairs(data_training, \n",
    "                  columns = 1:4, \n",
    "                  ggplot2::aes(colour = stage,alpha = 0.5), \n",
    "                  title=\"Figure 10: GGPairs plot of predictors\")\n",
    "ggpair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde3bd6-199e-4596-959b-f47ed35780da",
   "metadata": {},
   "source": [
    "We can see that for some predictors there are fairly distinct regions that distinguish between the two classes we are predicting. For example, patient's with mild heart disease tend to have a higher maximum heart rate and patient's who are older tend to have more cholesterol.\n",
    "\n",
    "The ggpairs plot also contains density plots to see how each the density of each predictor changes with class. This tells us on average, that patient's with mild heart disease are younger and patient's with a lower maximum heart rate have moderate/severe heart disease. Overall, our data should be sufficient in creating a viable model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8af6832-7f1a-4b67-bc01-8e9e6f3669d2",
   "metadata": {},
   "source": [
    "### Identifying the Ideal K Value\n",
    "\n",
    "To identify the ideal K value, we create a model for each $k$ from 1 to 100 and select the $K$ value that obtained the highest accuracy. First, we need to create a recipe to upsample the data. To upsample data, we decided to use the SMOTE (Synthetic Minority Over-sampling Technique) since we believed that it should produce more reliable and varied data. Nevertheless, upsampled data may be inaccurate so we upsampled only up to 95% of the observations of the majority class. This means the minority class will have 95% of the number of rows of the majority class. We also decided to use a seed of 69 to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7881961-9726-4578-9d30-28258aca4ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(69)\n",
    "\n",
    "recipe <- recipe(stage ~ age + chol + trestbps + thalach, data = data_training) |>\n",
    "    step_smote(stage, over_ratio=0.95) |>\n",
    "    step_scale(all_predictors()) |> \n",
    "    step_center(all_predictors()) \n",
    "\n",
    "data_testing_new <- recipe |>\n",
    "    prep() |>\n",
    "    bake(NULL) |>\n",
    "    group_by(stage) |>\n",
    "    summarize(count = n()) |>\n",
    "    ggplot(aes(stage, count)) +\n",
    "    geom_bar(stat = \"identity\") +\n",
    "    labs(x = \"Class\", y = \"Number of observations\") +\n",
    "    ggtitle(\"Figure 11: Number of Observations per Class in Upsampled Testing Set\")\n",
    "\n",
    "data_testing_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206e992-e251-4a80-8463-b2aa5daa8909",
   "metadata": {},
   "source": [
    "Now we can create our model. We decided to use a 10-fold cross validation as we believed this should be the best balance between accuracy and our computing power. Using a tibble with a single \"neighbors\" column containing values from 1 to 100, we created a model for each value and plotted its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b94b58-7e44-4362-813b-c187ab1a9782",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(69)\n",
    "\n",
    "k_vals <- tibble(neighbors = seq(1,100,1))\n",
    "\n",
    "# using the features in the project proposal\n",
    "tune_recipe <- recipe(stage ~ age + chol + trestbps + thalach, data = data_training) |>\n",
    "    step_smote(stage) |>\n",
    "    step_scale(all_predictors()) |> \n",
    "    step_center(all_predictors())\n",
    "\n",
    "tune_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) |>\n",
    "    set_engine(\"kknn\") |>\n",
    "    set_mode(\"classification\")\n",
    "\n",
    "tune_cv <- vfold_cv(data_training, v = 10, strata = stage)\n",
    "\n",
    "tune_wf <- workflow() |>\n",
    "    add_recipe(recipe) |>\n",
    "    add_model(tune_spec)\n",
    "\n",
    "tune_results <- tune_wf |>\n",
    "    tune_grid(resamples = tune_cv, grid = k_vals) |>\n",
    "    collect_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c55d3b-9998-4185-87dd-649fa4837146",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=7, repr.plot.height=5)\n",
    "\n",
    "\n",
    "k_vs_accuracy <- tune_results |> \n",
    "    filter(.metric == \"accuracy\") |> \n",
    "    ggplot(aes(neighbors, mean)) +\n",
    "    geom_point() +\n",
    "    geom_line() +\n",
    "    labs(x = \"Number of neighbors\", y = \"Accuracy\") +\n",
    "    ggtitle(\"Figure 12: The Effect of the Number of Neighbors on Model Accuracy\")\n",
    "\n",
    "k_vs_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3a50b1-f9ee-40e9-ae97-84ecfe955623",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Looking at the plot above, we can see that the accuracy of the model is the best when $k$ is roughly 40. We can narrow down the value we need by outputting the table and arranging the \"mean\" column in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b561b51b-c263-4ec8-8a45-ce909dec9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_k_table <- tune_results |> filter(.metric == \"accuracy\") |> arrange(desc(mean)) |> slice(1:5)\n",
    "best_k <- ideal_k_table |> slice(1) |> select(neighbors) |> pull()\n",
    "ideal_k_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f50fba-ae53-4ff4-95ce-e436d97cd62a",
   "metadata": {},
   "source": [
    "Figure 13\\\n",
    "We can see from the table above that 41 neighbors obtained the best accuracy of approximately 68%, meaning our ideal $k$ is 41. This $k$ value is saved to a variable called \"best_k\" for use later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab258289-e2bb-46c9-991d-0ac62eab5516",
   "metadata": {},
   "source": [
    "### Building New Model\n",
    "With our ideal $k$ value, we will build a new model and test it using our testing data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c67b2-e293-4cbf-968a-aa3c5dd74447",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_spec <- nearest_neighbor(weight_func= \"rectangular\", neighbors = best_k) |>\n",
    "    set_engine(\"kknn\") |>\n",
    "    set_mode(\"classification\")\n",
    "\n",
    "final_workflow <- workflow() |>\n",
    "    add_recipe(recipe)|>\n",
    "    add_model(final_spec) |>\n",
    "    fit(data = data_training)\n",
    "final_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ac9c0-fe5f-4729-90b6-c0fc65e575a9",
   "metadata": {},
   "source": [
    "### Testing New Model\n",
    "Using our new model, we can be assured that it has the ideal $k$ value for diagnosing heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c6cce8-d20f-4506-9f62-09618b740ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(69)\n",
    "\n",
    "predictions <- final_workflow |> predict(data_testing) |>\n",
    "               bind_cols(data_testing)\n",
    "\n",
    "# predictions \n",
    "metrics <- predictions |>\n",
    "    metrics(truth = stage, estimate = .pred_class) |>\n",
    "    filter(.metric == \"accuracy\")\n",
    "metrics \n",
    "\n",
    "f_score <- f_meas(predictions, stage, .pred_class)\n",
    "f_score\n",
    "\n",
    "confusion_matrix <- predictions |>\n",
    "    conf_mat(truth = stage, estimate = .pred_class)\n",
    "\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e21610-39c9-4a28-acd8-a4d291c27ee5",
   "metadata": {},
   "source": [
    "Figure 14\\\n",
    "Based on the metrics above, our model was only somewhat accurate when used against our testing set. It obtained an accuracy of 67.4%, which is almost the desired 70% model accuracy usually aimed for. However, the model also achieved an f-score of roughly 72% ...\n",
    "\n",
    "*Evaluate using accuracy, precision, recall, etc. Overdiagnose/Underdiagnose, how it can be used in the real-world etc.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74193e-5164-4c54-8918-f827211b405d",
   "metadata": {},
   "source": [
    "### Versatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4178481f-574b-433b-a48b-b4955b41181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(69)\n",
    "\n",
    "long_beach_data <- read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.va.data\",\n",
    "                          col_names = c(\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"))\n",
    "\n",
    "long_beach_data <- long_beach_data |> \n",
    "    mutate(age = as.integer(age), sex = as.factor(sex), cp = as.factor(cp), trestbps = as.integer(trestbps), \n",
    "    chol = as.integer(chol), fbs = as.factor(fbs), restecg = as.factor(restecg),  thalach = as.integer(thalach),\n",
    "    exang = as.factor(exang), oldpeak = as.integer(oldpeak), slope = as.factor(slope), ca = as.integer(ca), \n",
    "    thal = as.factor(thal), num = as.integer(num))  |> select(age, trestbps, chol, thalach, -ca, -thal, num) \n",
    "\n",
    "long_beach_data <- long_beach_data |> \n",
    "    filter(chol != 0) |> \n",
    "    filter(thalach != \"?\") |> \n",
    "    filter(trestbps != \"?\") |> \n",
    "    filter(num != \"?\") |>\n",
    "    mutate(stage = cut(num, breaks=c(-Inf,0,4), labels=c(\"Mild\",\"Moderate/Severe\")))\n",
    "\n",
    "balance <- long_beach_data |> \n",
    "    group_by(stage) |> \n",
    "    summarize(count = n())\n",
    "balance\n",
    "\n",
    "predictions <- final_workflow |> \n",
    "    predict(long_beach_data) |>\n",
    "    bind_cols(long_beach_data)\n",
    "\n",
    "# predictions \n",
    "metrics <- predictions |>\n",
    "    metrics(truth = stage, estimate = .pred_class) |>\n",
    "    filter(.metric == \"accuracy\") \n",
    "metrics\n",
    "\n",
    "# confusion matrix\n",
    "confusion_matrix <- predictions |>\n",
    "    conf_mat(truth = stage, estimate = .pred_class)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77540d11-8397-406e-969f-6bef8cff2798",
   "metadata": {},
   "source": [
    "This model has a higher accuracy than before but imbalanced classes. This suggests that our model is good at detecting moderate/severe cases of heart disease, but less good at detecting milder cases or cases where there isn't any heart disease. Additionally, this dataset has less data than the previous datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d4169-b0e9-4307-9451-3bc64de5e6c6",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af35dc-5d79-4d66-b9b8-3e79ef3060e7",
   "metadata": {},
   "source": [
    "### Summary of findings\n",
    "In summary, we found that we can predict the severity of heart disease with several predictors, with an accuracy of about 65%. We also tested our predictor on other datasets, which resulted in a slightly higher accuracy of 80%. In terms of precision and recall, the model performed okay on both. Since the model has a relatively low training and cross validation score, we can say that the best way to improve the model would probably be to increase the complexity of the model, because as of now it's not capturing all the variation in the dataset. \n",
    "\n",
    "### Expected findings\n",
    "We expected to find that the model would be able to account for a greater amount of variation in the dataset. One possible reason why it's not doing the best could be because we didn't include all the features or because we simplified the target label (reducing it to binary classification when we could've done multi-class classification). We chose to simplify the classification such that a value in the dataset of 0 meant that the label was mild, and a value of 1-4 meant that the label was moderate to severe; however, we could have also done multiple classes which might have a better accuracy result. Additionally, if we had access to all the features and didn't simplify them during the preprocessing step of the data pipeline, the model might have generalized better to the cross validation data. \n",
    "\n",
    "### Future impact\n",
    "If we can find a correlation between various factors and heart disease, this could impact future diagnosis, making the process faster and more accurate. A developed diagnosis pipeline could remove the need for a human doctor, which can be time consuming and expensive. Although the model isn't very accurate right now, a more complex model or a greater amount of data could lead to this model being available online for public use, allowing patients who are at risk for heart disease to find out sooner and with less hassle.\n",
    "\n",
    "### Future questions\n",
    "How can we increase the accuracy of the model, and how can this help patients with heart disease? How can changing the model architecture change it's performance on the dataset? How can the development of this model help more general cases of heart disease classification? How does the model perform when we provide more or less features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d48208-f8dc-490c-afa0-56acbb4e0243",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2c7c66-3c33-45ac-94c2-79445dc3335c",
   "metadata": {},
   "source": [
    "Carotid artery stenosis: Causes, symptoms and treatment. (n.d.). Cleveland Clinic. https://my.clevelandclinic.org/health/diseases/16845-carotid-artery-disease-carotid-artery-stenosis\n",
    "\n",
    "Janosi, A., Steinbrunn, W., Pfisterer, M., & Detrano, R. (n.d.). Heart Disease - UCI Machine Learning Repository. UC Irvine Machine Learning Repository. https://archive.ics.uci.edu/dataset/45/heart+disease \n",
    "\n",
    "Centers for Disease Control and Prevention. (2023, May 15). Heart disease facts. Centers for Disease Control and Prevention. https://www.cdc.gov/heartdisease/facts.htm "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
